{
 "metadata": {
  "name": "",
  "signature": "sha256:0df577aefee42f3ad89b4a6e60e02cacc29d9384afc64a97ad91f1e39f7d0918"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# BigQuery - Extracting Data\n",
      "\n",
      "This notebook demonstrates how you can run a SQL query to extract data from BigQuery and stream the results to a local file, or to a file in GCS, rather than into memory. This maybe useful for resultsets that are too large to directly fit into memory all at once.\n",
      "\n",
      "This notebook uses a sample dataset of request logs data from a web server.\n",
      "\n",
      "Related Links:\n",
      "\n",
      "* [BigQuery](https://cloud.google.com/bigquery/)\n",
      "* BigQuery [SQL reference](https://cloud.google.com/bigquery/query-reference)\n",
      "\n",
      "----\n",
      "\n",
      "NOTE:\n",
      "\n",
      "* If you're new to notebooks, or want to check out additional samples, check out the full [list](..) of notebooks."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gcp\n",
      "import gcp.bigquery as bq\n",
      "import gcp.storage as gcs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Querying BigQuery"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we'll define the query we want to use to retrieve results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%sql --name=data\n",
      "SELECT * FROM [cloud-datalab:sampledata.requestlogs_20140615] LIMIT 100000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets sample it, to get a sense of the data that will be extracted when the full query is executed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.sample()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "    <div class=\"bqtv\" id=\"bqtv_1436395676\"></div>\n",
        "    <div><br />job_9AeuAc7sSEHjQscYec1frcjvjDs<br />rows: 5</div>\n",
        "    <script>\n",
        "      require(['extensions/charting', 'element!bqtv_1436395676'],\n",
        "        function(charts, dom) {\n",
        "          charts.render(dom,\n",
        "            {\n",
        "              chartStyle:\"table\",\n",
        "              dataName:\"data-studio-team:_22cae82edc27e9be9a02c50fe9837efda6b0471d.anon2c032decd2c7375aea1e42951593ee63938937a2\",\n",
        "              fields:\"timestamp,latency,status,method,endpoint\",\n",
        "              totalRows:5,\n",
        "              rowsPerPage:25,\n",
        "            }, {}, {\"rows\": [{\"c\": [{\"v\": \"2014-06-15T07:00:00.003772\"}, {\"v\": 122}, {\"v\": 200}, {\"v\": \"GET\"}, {\"v\": \"Interact3\"}]}, {\"c\": [{\"v\": \"2014-06-15T07:00:00.428897\"}, {\"v\": 144}, {\"v\": 200}, {\"v\": \"GET\"}, {\"v\": \"Interact3\"}]}, {\"c\": [{\"v\": \"2014-06-15T07:00:00.536486\"}, {\"v\": 48}, {\"v\": 200}, {\"v\": \"GET\"}, {\"v\": \"Interact3\"}]}, {\"c\": [{\"v\": \"2014-06-15T07:00:00.652760\"}, {\"v\": 28}, {\"v\": 405}, {\"v\": \"GET\"}, {\"v\": \"Interact2\"}]}, {\"c\": [{\"v\": \"2014-06-15T07:00:00.670100\"}, {\"v\": 103}, {\"v\": 200}, {\"v\": \"GET\"}, {\"v\": \"Interact3\"}]}], \"cols\": [{\"type\": \"datetime\", \"id\": \"timestamp\", \"label\": \"timestamp\"}, {\"type\": \"number\", \"id\": \"latency\", \"label\": \"latency\"}, {\"type\": \"number\", \"id\": \"status\", \"label\": \"status\"}, {\"type\": \"string\", \"id\": \"method\", \"label\": \"method\"}, {\"type\": \"string\", \"id\": \"endpoint\", \"label\": \"endpoint\"}]});\n",
        "        }\n",
        "      );\n",
        "    </script>\n",
        "  "
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": []
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extracting Results to a Local File"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can use the `to_file` method on the query object to stream down results. Note that this can take some while for all the data to become available locally."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.to_file('/tmp/data.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "'/tmp/data.csv'"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To verify, lets open the file and read a few lines in. We expect to see the column headers in the first line, followed by a sample of individual data rows seen earlier."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('/tmp/data.csv') as datafile:\n",
      "    head = [next(datafile) for x in xrange(6)]\n",
      "print ''.join(head)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "timestamp,latency,status,method,endpoint\r\n",
        "2014-06-15 07:00:00.003772,122,200,GET,Interact3\r\n",
        "2014-06-15 07:00:00.428897,144,200,GET,Interact3\r\n",
        "2014-06-15 07:00:00.536486,48,200,GET,Interact3\r\n",
        "2014-06-15 07:00:00.652760,28,405,GET,Interact2\r\n",
        "2014-06-15 07:00:00.670100,103,200,GET,Interact3\r\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "ls -l /tmp/data.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-rw-r--r--  1 gram  wheel  4668222 Jul  8 15:48 /tmp/data.csv\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Extracting Results to a GCS Bucket"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Instead of streaming results into the local VM, it might be more useful to stream the results into GCS, and then read from GCS as needed."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the purposes of the sample, this code uses the current project id to create a reference to a GCS bucket that can be written to - specifically the same bucket that contains these sample notebooks. For the purposes of this job, we'll have BigQuery write out using 2 workers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gcs_bucket = gcp.Context.default().project_id + '-ipython'\n",
      "gcs_path_prefix = 'gs://%s/data/logs' % gcs_bucket"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data.extract([gcs_path_prefix + '-1-*.csv', gcs_path_prefix + '-2-*.csv'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "Job job_P3nevERXrQyYiwvxW1K1h0H-2oM"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once the job is complete, we can enumerate the GCS bucket to look for the written out file objects."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map(lambda item: item.key, list(gcs.bucket(gcs_bucket).items(prefix='data/')))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[u'data/',\n",
        " u'data/logs-1-000000000000.csv',\n",
        " u'data/logs-1-000000000001.csv',\n",
        " u'data/logs-2-000000000000.csv']"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}