{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the pygcp BigQuery APIs\n",
    "\n",
    "DataLab provides a set of Python libraries for accessing BigQuery in the context of notebooks that are different to the standard APIs (although largely functionally equivalent). This notebook will show you how to access the API documentation inside a notebook, and provide a summary of the API documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gcp.bigquery as bq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython autocomplete and object help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful ways to get help is to use IPython's autocomplete. If you have an object reference, you can enter '.' followed by TAB to get a list of possible completions. Try this below with the table object t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nikhilko-playground:test.nonexistent does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = bq.table('test.nonexistent')\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also type an object reference followed by a '?' to get more detailed help. Execute the cells below to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t.insertAll?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the cell magics %psource to see the source for an object, %pdoc to see its docstring, or %pdef for its signature.\n",
    "The first two will pop up a separate pane while the last will show as a cell output. You can try this out by executing the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      " "
     ]
    }
   ],
   "source": [
    "%pdef t.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdoc t.to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%psource t.to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you may want to access these without having a reference to the object. The classes used in the API are hidden behind constructor functions (e.g. Table is not exposed, but table() is). You can work around this if necessary by using the full path, which includes the private module containing the class; for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdoc bq._table.Table.to_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modules are documented below so you can easily see the names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery\n",
    "-------------------\n",
    "\n",
    "Google Cloud Platform library - BigQuery Functionality.\n",
    "\n",
    "Functions\n",
    "---------\n",
    "- **dataset** (name, context=None)\n",
    "\n",
    "    Returns the Dataset with the specified dataset_id.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * name: the name of the dataset, as a string or (project_id, dataset_id) tuple.\n",
    "\n",
    "    * context: an optional Context object providing project_id and credentials.  \n",
    "Returns:    \n",
    "A DataSet object.\n",
    "\n",
    "- **datasetname** (project_id, dataset_id)\n",
    "\n",
    "    Construct a DataSetName named tuple.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * project_id: the project ID\n",
    "\n",
    "    * dataset_id: the dataset ID  \n",
    "Returns:    \n",
    "A DataSetName named-tuple.\n",
    "\n",
    "- **datasets** (project_id=None, context=None)\n",
    "\n",
    "- **job** (job_id, context=None)\n",
    "\n",
    "    Create a job reference for a specific job ID.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * job_id: the job ID.  \n",
    "Returns:    \n",
    "A Job object.\n",
    "\n",
    "- **query** (sql_statement, context=None)\n",
    "\n",
    "    Creates a BigQuery query object.\n",
    "  \n",
    "    If a specific project id or credentials are unspecified, the default ones\n",
    "configured at the global level are used.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * sql_statement: the SQL query to execute.\n",
    "\n",
    "    * context: an optional Context object providing project_id and credentials.  \n",
    "Returns:    \n",
    "A query object that can be executed to retrieve data from BigQuery.\n",
    "\n",
    "- **query_job** (job_id, table, context=None)\n",
    "\n",
    "    Create a job reference for a specific query job ID.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * job_id: the job ID.\n",
    "\n",
    "    * table: the Table that will be used for the query results.  \n",
    "Returns:    \n",
    "A QueryJob object.\n",
    "\n",
    "- **schema** (data=None, definition=None)\n",
    "\n",
    "    Creates a table/view schema from its JSON representation, a list of data, or a Pandas\n",
    "dataframe.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * data: the Pandas Dataframe or list of data from which to infer the schema.\n",
    "\n",
    "    * definition: a definition of the schema as a list of dictionaries with 'name' and 'type' entries\n",
    "and possibly 'mode' and 'description' entries. Only used if no data argument was provided.\n",
    "'mode' can be 'NULLABLE', 'REQUIRED' or 'REPEATED'. For the allowed types, see:  \n",
    "\n",
    "    * https://cloud.google.com/bigquery/preparing-data-for-bigquery#datatypes  \n",
    "Returns:    \n",
    "A Schema object.\n",
    "\n",
    "- **sql** (sql_template, **kwargs)\n",
    "\n",
    "    Formats SQL templates by replacing placeholders with actual values.\n",
    "  \n",
    "    Placeholders in SQL are represented as $<name>. If '$' must appear within the  \n",
    "SQL statement literally, then it can be escaped as '$$'.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * sql_template: the template of the SQL statement with named placeholders.\n",
    "\n",
    "    * **kwargs: the dictionary of name/value pairs to use for placeholder values.  \n",
    "Returns:    \n",
    "The formatted SQL statement with placeholders replaced with their values.  \n",
    "Raises:    \n",
    "Exception if a placeholder was found in the SQL statement, but did not have\n",
    "a corresponding argument value.\n",
    "\n",
    "- **table** (name, context=None)\n",
    "\n",
    "    Creates a BigQuery table object.\n",
    "  \n",
    "    If a specific project id or credentials are unspecified, the default ones\n",
    "configured at the global level are used.\n",
    "  \n",
    "    The name must be a valid BigQuery table name, which is either\n",
    "\n",
    "    * project:dataset.table or dataset.table\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * name: the name of the table, as a string or (project_id, dataset_id, table_id) tuple.\n",
    "\n",
    "    * context: an optional Context object providing project_id and credentials.  \n",
    "Returns:    \n",
    "A Table object that can be used to retrieve table metadata from BigQuery.  \n",
    "Raises:    \n",
    "Exception if the name is invalid.\n",
    "\n",
    "- **tablename** (project_id, dataset_id, table_id)\n",
    "\n",
    "    Construct a TableName named tuple.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * project_id: the project ID\n",
    "\n",
    "    * dataset_id: the dataset ID\n",
    "\n",
    "    * table_id: tha Table ID  \n",
    "Returns:    \n",
    "A TableName named-tuple.\n",
    "\n",
    "- **udf** (inputs, outputs, implementation, context=None)\n",
    "\n",
    "    Creates a BigQuery SQL UDF query object.\n",
    "  \n",
    "    The implementation is a javascript function of the form:  \n",
    "function(row, emitFn) { ... }\n",
    "where the row matches a structure represented by inputs, and the emitFn\n",
    "is a function that accepts a structure represented by outputs.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * inputs: a list of (name, type) tuples representing the schema of input.\n",
    "\n",
    "    * outputs: a list of (name, type) tuples representing the schema of the output.\n",
    "\n",
    "    * implementation: a javascript function defining the UDF logic.\n",
    "\n",
    "    * context: an optional Context object providing project_id and credentials.\n",
    "\n",
    "- **view** (name, context=None)\n",
    "\n",
    "    Creates a BigQuery View object.\n",
    "  \n",
    "    If a specific project id or credentials are unspecified, the default ones\n",
    "configured at the global level are used.\n",
    "  \n",
    "    The name must be a valid BigQuery view name, which is either\n",
    "\n",
    "    * project:dataset.view or dataset.view\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * name: the name of the view, as a string or (project_id, dataset_id, view_id) tuple.\n",
    "\n",
    "    * context: an optional Context object providing project_id and credentials.  \n",
    "Returns:    \n",
    "A View object that can be used to retrieve table metadata from BigQuery.  \n",
    "Raises:    \n",
    "Exception if the name is invalid.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._dataset\n",
    "----------------------------\n",
    "\n",
    "Implements DataSet, and related DataSet BigQuery APIs.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### DataSet \n",
    "Represents a list of BigQuery tables in a dataset.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._dataset.DataSet\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **full_name**\n",
    "\n",
    "    The full name for the dataset.\n",
    "\n",
    "- **name**\n",
    "\n",
    "    The DataSetName for the dataset.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, name)\n",
    "\n",
    "    Initializes an instance of a DataSet.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests. The project ID will be inferred from\n",
    "this.\n",
    "\n",
    "    * name: the name of the dataset, as a string or (project_id, dataset_id) tuple.\n",
    "\n",
    "- **create** (self, friendly_name=None, description=None)\n",
    "\n",
    "    Creates the Dataset with the specified friendly name and description.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * friendly_name: (optional) the friendly name for the dataset if it is being created.\n",
    "\n",
    "    * description: (optional) a description for the dataset if it is being created.  \n",
    "Returns:    \n",
    "The DataSet.  \n",
    "Raises:    \n",
    "Exception if the DataSet could not be created.\n",
    "\n",
    "- **delete** (self, delete_contents=False)\n",
    "\n",
    "    Issues a request to delete the dataset.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * delete_contents: if True, any tables in the dataset will be deleted. If False and the\n",
    "dataset is non-empty an exception will be raised.  \n",
    "Returns:    \n",
    "None on success.  \n",
    "Raises:    \n",
    "Exception if the delete fails (including if table was nonexistent).\n",
    "\n",
    "- **exists** (self)\n",
    "\n",
    "    Checks if the dataset exists.\n",
    "  \n",
    "    Args:    \n",
    "None  \n",
    "Returns:    \n",
    "True if the dataset exists; False otherwise.\n",
    "\n",
    "#### DataSetLister \n",
    "Helper class for enumerating the datasets in a project.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._dataset.DataSetLister\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, project_id=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._job\n",
    "------------------------\n",
    "\n",
    "Implements BigQuery Job functionality.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### Job \n",
    "Represents a BigQuery Job.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._job.Job\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Descendents\n",
    "- gcp.bigquery._query_job.QueryJob\n",
    "\n",
    "##### Instance variables\n",
    "- **errors**\n",
    "\n",
    "    Get the errors in the job.\n",
    "  \n",
    "    Returns:    \n",
    "None if the job is still running, else the list of errors that occurred.\n",
    "\n",
    "- **failed**\n",
    "\n",
    "    Get the success state of the job.\n",
    "  \n",
    "    Returns:    \n",
    "True if the job failed; False if it is still running or succeeded (possibly with partial\n",
    "failure).\n",
    "\n",
    "- **fatal_error**\n",
    "\n",
    "    Get the job error.\n",
    "  \n",
    "    Returns:    \n",
    "None if the job succeeded or is still running, else the error tuple for the failure.\n",
    "\n",
    "- **id**\n",
    "\n",
    "    Get the Job ID.\n",
    "  \n",
    "    Returns:    \n",
    "The ID of the job.\n",
    "\n",
    "- **iscomplete**\n",
    "\n",
    "    Get the completion state of the job.\n",
    "  \n",
    "    Returns:    \n",
    "True if the job is complete; False if it is still running.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, job_id)\n",
    "\n",
    "    Initializes an instance of a Job.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests. The project ID will be inferred from\n",
    "this.\n",
    "\n",
    "    * job_id: the BigQuery job ID corresponding to this job.\n",
    "\n",
    "- **wait** (self, timeout=None, poll=5)\n",
    "\n",
    "    Wait for the job to complete, or a timeout to happen.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * timeout: how long to poll before giving up; default None which means no timeout.\n",
    "\n",
    "    * poll: interval in seconds between polls (default 5)\n",
    "  \n",
    "    Returns:    \n",
    "True if job completed; False if wait timed out.\n",
    "\n",
    "#### JobError \n",
    "JobError(location, message, reason)\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._job.JobError\n",
    "\n",
    "- __builtin__.tuple\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **location**\n",
    "\n",
    "    Alias for field number 0\n",
    "\n",
    "- **message**\n",
    "\n",
    "    Alias for field number 1\n",
    "\n",
    "- **reason**\n",
    "\n",
    "    Alias for field number 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._query_job\n",
    "------------------------------\n",
    "\n",
    "Implements BigQuery query job functionality.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### QueryJob \n",
    "Represents a BigQuery Query Job.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._query_job.QueryJob\n",
    "\n",
    "- gcp.bigquery._job.Job\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **errors**\n",
    "\n",
    "    Get the errors in the job.\n",
    "  \n",
    "    Returns:    \n",
    "None if the job is still running, else the list of errors that occurred.\n",
    "\n",
    "- **failed**\n",
    "\n",
    "    Get the success state of the job.\n",
    "  \n",
    "    Returns:    \n",
    "True if the job failed; False if it is still running or succeeded (possibly with partial\n",
    "failure).\n",
    "\n",
    "- **fatal_error**\n",
    "\n",
    "    Get the job error.\n",
    "  \n",
    "    Returns:    \n",
    "None if the job succeeded or is still running, else the error tuple for the failure.\n",
    "\n",
    "- **id**\n",
    "\n",
    "    Get the Job ID.\n",
    "  \n",
    "    Returns:    \n",
    "The ID of the job.\n",
    "\n",
    "- **iscomplete**\n",
    "\n",
    "    Get the completion state of the job.\n",
    "  \n",
    "    Returns:    \n",
    "True if the job is complete; False if it is still running.\n",
    "\n",
    "- **results**\n",
    "\n",
    "    Get the table used for the results of the query. If the query is incomplete, this blocks.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * timeout: timeout in msec to wait for the query to complete.\n",
    "  \n",
    "    Raises:    \n",
    "Exception if we timed out waiting for results.\n",
    "\n",
    "- **sql**\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, job_id, table_name, sql, timeout=0)\n",
    "\n",
    "- **wait** (self, timeout=None, poll=5)\n",
    "\n",
    "    Wait for the job to complete, or a timeout to happen.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * timeout: how long to poll before giving up; default None which means no timeout.\n",
    "\n",
    "    * poll: interval in seconds between polls (default 5)\n",
    "  \n",
    "    Returns:    \n",
    "True if job completed; False if wait timed out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._query_results_table\n",
    "----------------------------------------\n",
    "\n",
    "Implements BigQuery query job results table functionality.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### QueryResultsTable \n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._query_results_table.QueryResultsTable\n",
    "\n",
    "- gcp.bigquery._table.Table\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **full_name**\n",
    "\n",
    "    The full name for the table.\n",
    "\n",
    "- **is_temporary**\n",
    "\n",
    "    Whether this is a short-lived table or not.\n",
    "\n",
    "- **job_id**\n",
    "\n",
    "- **length**\n",
    "\n",
    "    Get the length of the table (number of rows). We don't use __len__ as this may\n",
    "return -1 for 'unknown'.\n",
    "\n",
    "- **metadata**\n",
    "\n",
    "    Retrieves metadata about the table.\n",
    "  \n",
    "    Returns:    \n",
    "A TableMetadata object.  \n",
    "Raises  \n",
    "Exception if the request could not be executed or the response was malformed.\n",
    "\n",
    "- **name**\n",
    "\n",
    "    The TableName for the table.\n",
    "\n",
    "- **schema**\n",
    "\n",
    "    Retrieves the schema of the table.\n",
    "  \n",
    "    Returns:    \n",
    "A Schema object containing a list of schema fields and associated metadata.  \n",
    "Raises  \n",
    "Exception if the request could not be executed or the response was malformed.\n",
    "\n",
    "- **sql**\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, name, job, is_temporary=False)\n",
    "\n",
    "    Initializes an instance of a Table object.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * name: the name of the table either as a string or a 3-part tuple (projectid, datasetid, name).\n",
    "\n",
    "    * is_temporary: if True, this is a short-lived table for intermediate results (default False).\n",
    "\n",
    "- **create** (self, schema, overwrite=False)\n",
    "\n",
    "    Create the table with the specified schema.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * schema: the schema to use to create the table. Should be a list of dictionaries, each\n",
    "containing at least a pair of entries, 'name' and 'type'.  \n",
    "\n",
    "    * See https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n",
    "\n",
    "    * overwrite: if True, delete the object first if it exists. If False and the object exists,\n",
    "creation will fail and raise an Exception.  \n",
    "Returns:    \n",
    "The Table instance.  \n",
    "Raises:    \n",
    "Exception if the table couldn't be created or already exists and truncate was False.\n",
    "\n",
    "- **delete** (self)\n",
    "\n",
    "    Delete the table.\n",
    "  \n",
    "    Returns:    \n",
    "Nothing\n",
    "\n",
    "- **exists** (self)\n",
    "\n",
    "    Checks if the table exists.\n",
    "  \n",
    "    Returns:    \n",
    "True if the table exists; False otherwise.  \n",
    "Raises:    \n",
    "Exception if there was an error requesting information about the table.\n",
    "\n",
    "- **extract** (self, destination, format='CSV', compress=False, field_delimiter=',', print_header=True, timeout=None, poll=5)\n",
    "\n",
    "    Exports the table to GCS.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * destination: the destination URI(s). Can be a single URI or a list.\n",
    "\n",
    "    * format: the format to use for the exported data; one of CSV, NEWLINE_DELIMITED_JSON or AVRO.  \n",
    "Defaults to CSV.\n",
    "compress whether to compress the data on export. Compression is not supported for  \n",
    "AVRO format. Defaults to False.\n",
    "\n",
    "    * field_delimiter: for CSV exports, the field delimiter to use. Defaults to ','\n",
    "\n",
    "    * print_header: for CSV exports, whether to include an initial header line. Default true.\n",
    "\n",
    "    * timeout: how long to block waiting for the job to complete; default None which means no\n",
    "limit. If not None, then the call may return an incomplete Job which will have to be\n",
    "checked for completion using Job.wait or Job.iscomplete.\n",
    "\n",
    "    * poll: interval in seconds between job status polls (default 5).  \n",
    "Returns:    \n",
    "A Job object for the export Job if it was started successfully; else None.\n",
    "\n",
    "- **insertAll** (self, data, include_index=False, index_name=None)\n",
    "\n",
    "    Insert the contents of a Pandas DataFrame or a list of dictionaries into the table.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * data: the DataFrame or list to insert.\n",
    "\n",
    "    * include_index: whether to include the DataFrame or list index as a column in the BQ table.\n",
    "\n",
    "    * index_name: for a list, if include_index is True, this should be the name for the index.  \n",
    "If not specified, 'Index' will be used.  \n",
    "Returns:    \n",
    "The table.  \n",
    "Raises:    \n",
    "Exception if the table doesn't exist, the schema differs from the data's schema, or the insert\n",
    "failed.\n",
    "\n",
    "- **load** (self, source, append=False, overwrite=False, source_format='CSV', field_delimiter=',', allow_jagged_rows=False, allow_quoted_newlines=False, encoding='UTF-8', ignore_unknown_values=False, max_bad_records=0, quote='\"', skip_leading_rows=0)\n",
    "\n",
    "    Load the table from GCS.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * source: the URL of the source bucket(s). Can include wildcards.\n",
    "\n",
    "    * append: if True append onto existing table contents.\n",
    "\n",
    "    * overwrite: if True overwrite existing table contents.\n",
    "\n",
    "    * source_format: the format of the data; default 'CSV'. Other options are DATASTORE_BACKUP\n",
    "or NEWLINE_DELIMITED_JSON.\n",
    "\n",
    "    * field_delimiter: The separator for fields in a CSV file. BigQuery converts the string to  \n",
    "ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the data\n",
    "as raw binary (default ',').\n",
    "\n",
    "    * allow_jagged_rows: If True, accept rows in CSV files that are missing trailing optional\n",
    "columns; the missing values are treated as nulls (default False).\n",
    "\n",
    "    * allow_quoted_newlines: If True, allow quoted data sections in CSV files that contain newline\n",
    "characters (default False).\n",
    "\n",
    "    * encoding: The character encoding of the data, either 'UTF-8' (the default) or 'ISO-8859-1'.\n",
    "\n",
    "    * ignore_unknown_values: If True, accept rows that contain values that do not match the schema;\n",
    "the unknown values are ignored (default False).\n",
    "max_bad_records The maximum number of bad records that are allowed (and ignored) before\n",
    "returning an 'invalid' error in the Job result (default 0).\n",
    "\n",
    "    * quote: The value used to quote data sections in a CSV file; default '\"'. If your data does\n",
    "not contain quoted sections, set the property value to an empty string. If your data\n",
    "contains quoted newline characters, you must also enable allow_quoted_newlines.\n",
    "\n",
    "    * skip_leading_rows: A number of rows at the top of a CSV file to skip (default 0).\n",
    "  \n",
    "    Returns:    \n",
    "A Job object for the load Job if it was started successfully; else None.\n",
    "\n",
    "- **range** (self, start_row=0, max_rows=None)\n",
    "\n",
    "    Get an iterator to iterate through a set of table rows.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * start_row: the row of the table at which to start the iteration (default 0)\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to iterate through (default None)\n",
    "  \n",
    "    Returns:    \n",
    "A row iterator.\n",
    "\n",
    "- **sample** (self, fields=None, count=5, sampling=None, timeout=0, use_cache=True)\n",
    "\n",
    "    Retrieves a sampling of data from the table.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: an optional count of rows to retrieve which is used if a specific\n",
    "sampling is not specified.\n",
    "\n",
    "    * sampling: an optional sampling strategy to apply to the table.\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not.  \n",
    "Returns:    \n",
    "A QueryResults object containing the resulting data.  \n",
    "Raises:    \n",
    "Exception if the sample query could not be executed or query response was malformed.\n",
    "\n",
    "- **to_dataframe** (self, start_row=0, max_rows=None)\n",
    "\n",
    "    Exports the table to a Pandas dataframe.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * start_row: the row of the table at which to start the export (default 0)\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to export (default None)  \n",
    "Returns:    \n",
    "A dataframe containing the table data.\n",
    "\n",
    "- **to_file** (self, path, start_row=0, max_rows=None, write_header=True, dialect=<class csv.excel at 0x1042686d0>)\n",
    "\n",
    "    Save the results to a local file in CSV format.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * path: path on the local filesystem for the saved results.\n",
    "\n",
    "    * start_row: the row of the table at which to start the export (default 0)\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to export (default None)\n",
    "\n",
    "    * write_header: if true (the default), write column name header row at start of file\n",
    "\n",
    "    * dialect: the format to use for the output. By default, csv.excel. See\n",
    "\n",
    "    * https://docs.python.org/2/library/csv.html#csv-fmt-params for how to customize this.  \n",
    "Raises:    \n",
    "An Exception if the operation failed.\n",
    "\n",
    "- **update** (self, friendly_name=None, description=None, expiry=None, schema=None)\n",
    "\n",
    "    Selectively updates Table information.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * friendly_name: if not None, the new friendly name.\n",
    "\n",
    "    * description: if not None, the new description.\n",
    "\n",
    "    * expiry: if not None, the new expiry time, either as a DateTime or milliseconds since epoch.\n",
    "\n",
    "    * schema: if not None, the new schema: either a list of dictionaries or a Schema.\n",
    "  \n",
    "    Returns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._query\n",
    "--------------------------\n",
    "\n",
    "Implements Query BigQuery API.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### Query \n",
    "Represents a Query object that encapsulates a BigQuery SQL query.\n",
    "  \n",
    "This object can be used to execute SQL queries and retrieve results.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._query.Query\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Static methods\n",
    "- **sampling_query** (api, sql, fields=None, count=5, sampling=None)\n",
    "\n",
    "    Returns a sampling Query for the SQL object.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * sql: the SQL object to sample\n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: an optional count of rows to retrieve which is used if a specific\n",
    "sampling is not specified.\n",
    "\n",
    "    * sampling: an optional sampling strategy to apply to the table.  \n",
    "Returns:    \n",
    "A Query object for sampling the table.\n",
    "\n",
    "##### Instance variables\n",
    "- **sql**\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, sql)\n",
    "\n",
    "    Initializes an instance of a Query object.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * sql: the BigQuery SQL string to execute.\n",
    "\n",
    "- **execute** (self, table_name=None, append=False, overwrite=False, use_cache=True, batch=True, timeout=0)\n",
    "\n",
    "    Initiate the query.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * dataset_id: the datasetId for the result table.\n",
    "\n",
    "    * table_name: the result table name as a string or TableName; if None (the default), then a\n",
    "temporary table will be used.\n",
    "\n",
    "    * append: if True, append to the table if it is non-empty; else the request will fail if table\n",
    "is non-empty unless overwrite is True (default False).\n",
    "\n",
    "    * overwrite: if the table already exists, truncate it instead of appending or raising an  \n",
    "Exception (default False).\n",
    "\n",
    "    * use_cache: whether to use past query results or ignore cache. Has no effect if destination is\n",
    "specified (default True).\n",
    "\n",
    "    * batch: whether to run this as a batch job (lower priority) or as an interactive job (high\n",
    "priority, more expensive) (default True).\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete (default 0).  \n",
    "Returns:    \n",
    "A Job for the query  \n",
    "Raises:    \n",
    "Exception (KeyError) if query could not be executed.\n",
    "\n",
    "- **extract** (self, destination, format='CSV', compress=False, field_delimiter=',', print_header=True, use_cache=True, timeout=0, extract_timeout=None, poll=5)\n",
    "\n",
    "    Exports the query results to GCS.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * destination: the destination URI(s). Can be a single URI or a list.\n",
    "\n",
    "    * format: the format to use for the exported data; one of CSV, NEWLINE_DELIMITED_JSON or AVRO\n",
    "(default 'CSV').\n",
    "compress whether to compress the data on export. Compression is not supported for  \n",
    "AVRO format (default False).\n",
    "\n",
    "    * field_delimiter: for CSV exports, the field delimiter to use (default ',').\n",
    "\n",
    "    * print_header: for CSV exports, whether to include an initial header line (default True).\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete (default 0).\n",
    "\n",
    "    * use_cache: whether to use cached results or not (default True).\n",
    "\n",
    "    * extract_timeout: how long to block waiting for the extract job to complete; default None\n",
    "which means no limit. If not None, then the call may return an incomplete Job which will\n",
    "have to be checked for completion using Job.wait or Job.iscomplete.\n",
    "\n",
    "    * poll: interval in seconds between job status polls (default 5).  \n",
    "Returns:    \n",
    "A Job object for the export Job if it was started successfully; else None.  \n",
    "Raises:    \n",
    "An Exception if the query timed out or failed.\n",
    "\n",
    "- **results** (self, timeout=0, use_cache=True)\n",
    "\n",
    "    Retrieves results for the query.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not. Ignored if append is specified.  \n",
    "Returns:    \n",
    "A QueryResultsTable containing the result set.  \n",
    "Raises:    \n",
    "Exception if the query could not be executed or query response was\n",
    "malformed.\n",
    "\n",
    "- **sample** (self, count=5, fields=None, sampling=None, timeout=0, use_cache=True)\n",
    "\n",
    "    Retrieves a sampling of rows for the query.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * count: an optional count of rows to retrieve which is used if a specific\n",
    "sampling is not specified (default 5).\n",
    "\n",
    "    * fields: the list of fields to sample (default None implies all).\n",
    "\n",
    "    * sampling: an optional sampling strategy to apply to the table.\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete (default 0).\n",
    "\n",
    "    * use_cache: whether to use cached results or not (default True).  \n",
    "Returns:    \n",
    "A QueryResultsTable containing a sampling of the result set.  \n",
    "Raises:    \n",
    "Exception if the query could not be executed or query response was malformed.\n",
    "\n",
    "- **save_as_view** (self, view_name)\n",
    "\n",
    "    Create a View from this Query.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * view_name: the name of the View either as a string or a 3-part tuple\n",
    "(projectid, datasetid, name).\n",
    "  \n",
    "    Returns:    \n",
    "A View for the Query.\n",
    "\n",
    "- **to_dataframe** (self, start_row=0, max_rows=None, use_cache=True, timeout=0)\n",
    "\n",
    "    Exports the query results to a Pandas dataframe.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * start_row: the row of the table at which to start the export (default 0).\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to export (default None).\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete (default 0).\n",
    "\n",
    "    * use_cache: whether to use cached results or not (default True).  \n",
    "Returns:    \n",
    "A dataframe containing the table data.\n",
    "\n",
    "- **to_file** (self, path, start_row=0, max_rows=None, timeout=0, use_cache=True, write_header=True, dialect=<class csv.excel at 0x10519a6d0>)\n",
    "\n",
    "    Save the results to a local file in CSV format.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * path: path on the local filesystem for the saved results.\n",
    "\n",
    "    * start_row: the row of the table at which to start the export (default 0).\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to export (default None).\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not.\n",
    "\n",
    "    * write_header: if true (the default), write column name header row at start of file.\n",
    "\n",
    "    * dialect: the format to use for the output. By default, csv.excel. See\n",
    "\n",
    "    * https://docs.python.org/2/library/csv.html#csv-fmt-params for how to customize this.  \n",
    "Returns:    \n",
    "The path to the local file.  \n",
    "Raises:    \n",
    "An Exception if the operation failed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._sampling\n",
    "-----------------------------\n",
    "\n",
    "Implements BigQuery related data sampling strategies.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### Sampling \n",
    "Provides common sampling strategies.\n",
    "  \n",
    "Sampling strategies can be used for sampling tables or queries.\n",
    "  \n",
    "They are implemented as functions that take in a SQL statement representing the table or query\n",
    "that should be sampled, and return a new SQL statement that limits the result set in some manner.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._sampling.Sampling\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Static methods\n",
    "- **default** (fields=None, count=5)\n",
    "\n",
    "    Provides a simple default sampling strategy which limits the result set by a count.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: optional number of rows to limit the sampled results to.  \n",
    "Returns:    \n",
    "A sampling function that can be applied to get a random sampling.\n",
    "\n",
    "- **hashed** (field_name, percent, fields=None, count=0)\n",
    "\n",
    "    Provides a sampling strategy based on hashing and selecting a percentage of data.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * field_name: the name of the field to hash.\n",
    "\n",
    "    * percent: the percentage of the resulting hashes to select.\n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: optional maximum count of rows to pick.  \n",
    "Returns:    \n",
    "A sampling function that can be applied to get a hash-based sampling.\n",
    "\n",
    "- **sorted** (field_name, ascending=True, fields=None, count=5)\n",
    "\n",
    "    Provides a sampling strategy that picks from an ordered set of rows.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * field_name: the name of the field to sort the rows by.\n",
    "\n",
    "    * ascending: whether to sort in ascending direction or not.\n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: optional number of rows to limit the sampled results to.  \n",
    "Returns:    \n",
    "A sampling function that can be applied to get the initial few rows.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._table\n",
    "--------------------------\n",
    "\n",
    "Implements Table, and related Table BigQuery APIs.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### Schema \n",
    "Represents the schema of a BigQuery table.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._table.Schema\n",
    "\n",
    "- __builtin__.list\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, data=None, definition=None)\n",
    "\n",
    "    Initializes a Schema from its raw JSON representation, a Pandas Dataframe, or a list.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * data: A Pandas DataFrame or a list of dictionaries or lists from which to infer a schema.\n",
    "\n",
    "    * definition: a definition of the schema as a list of dictionaries with 'name' and 'type'\n",
    "entries and possibly 'mode' and 'description' entries. Only used if no data argument was\n",
    "provided. 'mode' can be 'NULLABLE', 'REQUIRED' or 'REPEATED'. For the allowed types, see:  \n",
    "\n",
    "    * https://cloud.google.com/bigquery/preparing-data-for-bigquery#datatypes\n",
    "\n",
    "#### Table \n",
    "Represents a Table object referencing a BigQuery table. \n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._table.Table\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Descendents\n",
    "- gcp.bigquery._query_results_table.QueryResultsTable\n",
    "\n",
    "##### Instance variables\n",
    "- **full_name**\n",
    "\n",
    "    The full name for the table.\n",
    "\n",
    "- **is_temporary**\n",
    "\n",
    "    Whether this is a short-lived table or not.\n",
    "\n",
    "- **length**\n",
    "\n",
    "    Get the length of the table (number of rows). We don't use __len__ as this may\n",
    "return -1 for 'unknown'.\n",
    "\n",
    "- **metadata**\n",
    "\n",
    "    Retrieves metadata about the table.\n",
    "  \n",
    "    Returns:    \n",
    "A TableMetadata object.  \n",
    "Raises  \n",
    "Exception if the request could not be executed or the response was malformed.\n",
    "\n",
    "- **name**\n",
    "\n",
    "    The TableName for the table.\n",
    "\n",
    "- **schema**\n",
    "\n",
    "    Retrieves the schema of the table.\n",
    "  \n",
    "    Returns:    \n",
    "A Schema object containing a list of schema fields and associated metadata.  \n",
    "Raises  \n",
    "Exception if the request could not be executed or the response was malformed.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, name)\n",
    "\n",
    "    Initializes an instance of a Table object.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * name: the name of the table either as a string or a 3-part tuple (projectid, datasetid, name).\n",
    "\n",
    "- **create** (self, schema, overwrite=False)\n",
    "\n",
    "    Create the table with the specified schema.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * schema: the schema to use to create the table. Should be a list of dictionaries, each\n",
    "containing at least a pair of entries, 'name' and 'type'.  \n",
    "\n",
    "    * See https://cloud.google.com/bigquery/docs/reference/v2/tables#resource\n",
    "\n",
    "    * overwrite: if True, delete the object first if it exists. If False and the object exists,\n",
    "creation will fail and raise an Exception.  \n",
    "Returns:    \n",
    "The Table instance.  \n",
    "Raises:    \n",
    "Exception if the table couldn't be created or already exists and truncate was False.\n",
    "\n",
    "- **delete** (self)\n",
    "\n",
    "    Delete the table.\n",
    "  \n",
    "    Returns:    \n",
    "Nothing\n",
    "\n",
    "- **exists** (self)\n",
    "\n",
    "    Checks if the table exists.\n",
    "  \n",
    "    Returns:    \n",
    "True if the table exists; False otherwise.  \n",
    "Raises:    \n",
    "Exception if there was an error requesting information about the table.\n",
    "\n",
    "- **extract** (self, destination, format='CSV', compress=False, field_delimiter=',', print_header=True, timeout=None, poll=5)\n",
    "\n",
    "    Exports the table to GCS.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * destination: the destination URI(s). Can be a single URI or a list.\n",
    "\n",
    "    * format: the format to use for the exported data; one of CSV, NEWLINE_DELIMITED_JSON or AVRO.  \n",
    "Defaults to CSV.\n",
    "compress whether to compress the data on export. Compression is not supported for  \n",
    "AVRO format. Defaults to False.\n",
    "\n",
    "    * field_delimiter: for CSV exports, the field delimiter to use. Defaults to ','\n",
    "\n",
    "    * print_header: for CSV exports, whether to include an initial header line. Default true.\n",
    "\n",
    "    * timeout: how long to block waiting for the job to complete; default None which means no\n",
    "limit. If not None, then the call may return an incomplete Job which will have to be\n",
    "checked for completion using Job.wait or Job.iscomplete.\n",
    "\n",
    "    * poll: interval in seconds between job status polls (default 5).  \n",
    "Returns:    \n",
    "A Job object for the export Job if it was started successfully; else None.\n",
    "\n",
    "- **insertAll** (self, data, include_index=False, index_name=None)\n",
    "\n",
    "    Insert the contents of a Pandas DataFrame or a list of dictionaries into the table.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * data: the DataFrame or list to insert.\n",
    "\n",
    "    * include_index: whether to include the DataFrame or list index as a column in the BQ table.\n",
    "\n",
    "    * index_name: for a list, if include_index is True, this should be the name for the index.  \n",
    "If not specified, 'Index' will be used.  \n",
    "Returns:    \n",
    "The table.  \n",
    "Raises:    \n",
    "Exception if the table doesn't exist, the schema differs from the data's schema, or the insert\n",
    "failed.\n",
    "\n",
    "- **load** (self, source, append=False, overwrite=False, source_format='CSV', field_delimiter=',', allow_jagged_rows=False, allow_quoted_newlines=False, encoding='UTF-8', ignore_unknown_values=False, max_bad_records=0, quote='\"', skip_leading_rows=0)\n",
    "\n",
    "    Load the table from GCS.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * source: the URL of the source bucket(s). Can include wildcards.\n",
    "\n",
    "    * append: if True append onto existing table contents.\n",
    "\n",
    "    * overwrite: if True overwrite existing table contents.\n",
    "\n",
    "    * source_format: the format of the data; default 'CSV'. Other options are DATASTORE_BACKUP\n",
    "or NEWLINE_DELIMITED_JSON.\n",
    "\n",
    "    * field_delimiter: The separator for fields in a CSV file. BigQuery converts the string to  \n",
    "ISO-8859-1 encoding, and then uses the first byte of the encoded string to split the data\n",
    "as raw binary (default ',').\n",
    "\n",
    "    * allow_jagged_rows: If True, accept rows in CSV files that are missing trailing optional\n",
    "columns; the missing values are treated as nulls (default False).\n",
    "\n",
    "    * allow_quoted_newlines: If True, allow quoted data sections in CSV files that contain newline\n",
    "characters (default False).\n",
    "\n",
    "    * encoding: The character encoding of the data, either 'UTF-8' (the default) or 'ISO-8859-1'.\n",
    "\n",
    "    * ignore_unknown_values: If True, accept rows that contain values that do not match the schema;\n",
    "the unknown values are ignored (default False).\n",
    "max_bad_records The maximum number of bad records that are allowed (and ignored) before\n",
    "returning an 'invalid' error in the Job result (default 0).\n",
    "\n",
    "    * quote: The value used to quote data sections in a CSV file; default '\"'. If your data does\n",
    "not contain quoted sections, set the property value to an empty string. If your data\n",
    "contains quoted newline characters, you must also enable allow_quoted_newlines.\n",
    "\n",
    "    * skip_leading_rows: A number of rows at the top of a CSV file to skip (default 0).\n",
    "  \n",
    "    Returns:    \n",
    "A Job object for the load Job if it was started successfully; else None.\n",
    "\n",
    "- **range** (self, start_row=0, max_rows=None)\n",
    "\n",
    "    Get an iterator to iterate through a set of table rows.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * start_row: the row of the table at which to start the iteration (default 0)\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to iterate through (default None)\n",
    "  \n",
    "    Returns:    \n",
    "A row iterator.\n",
    "\n",
    "- **sample** (self, fields=None, count=5, sampling=None, timeout=0, use_cache=True)\n",
    "\n",
    "    Retrieves a sampling of data from the table.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: an optional count of rows to retrieve which is used if a specific\n",
    "sampling is not specified.\n",
    "\n",
    "    * sampling: an optional sampling strategy to apply to the table.\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not.  \n",
    "Returns:    \n",
    "A QueryResults object containing the resulting data.  \n",
    "Raises:    \n",
    "Exception if the sample query could not be executed or query response was malformed.\n",
    "\n",
    "- **to_dataframe** (self, start_row=0, max_rows=None)\n",
    "\n",
    "    Exports the table to a Pandas dataframe.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * start_row: the row of the table at which to start the export (default 0)\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to export (default None)  \n",
    "Returns:    \n",
    "A dataframe containing the table data.\n",
    "\n",
    "- **to_file** (self, path, start_row=0, max_rows=None, write_header=True, dialect=<class csv.excel at 0x10519a6d0>)\n",
    "\n",
    "    Save the results to a local file in CSV format.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * path: path on the local filesystem for the saved results.\n",
    "\n",
    "    * start_row: the row of the table at which to start the export (default 0)\n",
    "\n",
    "    * max_rows: an upper limit on the number of rows to export (default None)\n",
    "\n",
    "    * write_header: if true (the default), write column name header row at start of file\n",
    "\n",
    "    * dialect: the format to use for the output. By default, csv.excel. See\n",
    "\n",
    "    * https://docs.python.org/2/library/csv.html#csv-fmt-params for how to customize this.  \n",
    "Raises:    \n",
    "An Exception if the operation failed.\n",
    "\n",
    "- **update** (self, friendly_name=None, description=None, expiry=None, schema=None)\n",
    "\n",
    "    Selectively updates Table information.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * friendly_name: if not None, the new friendly name.\n",
    "\n",
    "    * description: if not None, the new description.\n",
    "\n",
    "    * expiry: if not None, the new expiry time, either as a DateTime or milliseconds since epoch.\n",
    "\n",
    "    * schema: if not None, the new schema: either a list of dictionaries or a Schema.\n",
    "  \n",
    "    Returns:\n",
    "\n",
    "#### TableMetadata \n",
    "Represents metadata about a BigQuery table.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._table.TableMetadata\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **created_on**\n",
    "\n",
    "    The creation timestamp.\n",
    "\n",
    "- **description**\n",
    "\n",
    "    The description of the table if it exists.\n",
    "\n",
    "- **expires_on**\n",
    "\n",
    "    The timestamp for when the table will expire.\n",
    "\n",
    "- **friendly_name**\n",
    "\n",
    "    The friendly name of the table if it exists.\n",
    "\n",
    "- **full_name**\n",
    "\n",
    "    The full name of the table.\n",
    "\n",
    "- **modified_on**\n",
    "\n",
    "    The timestamp for when the table was last modified.\n",
    "\n",
    "- **rows**\n",
    "\n",
    "    The number of rows within the table.\n",
    "\n",
    "- **size**\n",
    "\n",
    "    The size of the table in bytes.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, table, info)\n",
    "\n",
    "    Initializes an instance of a TableMetadata.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * table: the table this belongs to.\n",
    "\n",
    "    * info: The BigQuery information about this table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._udf\n",
    "------------------------\n",
    "\n",
    "Google Cloud Platform library - BigQuery UDF Functionality.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### Function \n",
    "Represents a BigQuery UDF declaration.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._udf.Function\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, inputs, outputs, implementation)\n",
    "\n",
    "    Initializes a Function object from its pieces.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * inputs: a list of string field names representing the schema of input.\n",
    "\n",
    "    * outputs: a list of name/type tuples representing the schema of the output.\n",
    "\n",
    "    * implementation: a javascript function implementing the logic.\n",
    "\n",
    "#### FunctionCall \n",
    "Represents a BigQuery UDF invocation.\n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._udf.FunctionCall\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **sql**\n",
    "\n",
    "    Gets the underlying SQL representation of this UDF object.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, data, inputs, outputs, implementation)\n",
    "\n",
    "    Initializes a UDF object from its pieces.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * data: the query or table over which the UDF operates.\n",
    "\n",
    "    * inputs: a list of string field names representing the schema of input.\n",
    "\n",
    "    * outputs: a list of name/type tuples representing the schema of the output.\n",
    "\n",
    "    * implementation: a javascript function implementing the logic.\n",
    "\n",
    "- **results** (self, timeout=0, use_cache=True)\n",
    "\n",
    "    Retrieves results from executing the UDF.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not.  \n",
    "Returns:    \n",
    "A QueryResults objects representing the result set.  \n",
    "Raises:    \n",
    "Exception if the query could not be executed or query response was malformed.\n",
    "\n",
    "#### FunctionEvaluation \n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._udf.FunctionEvaluation\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **data**\n",
    "\n",
    "- **implementation**\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, implementation, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module gcp.bigquery._view\n",
    "-------------------------\n",
    "\n",
    "Implements BigQuery Views.\n",
    "\n",
    "Classes\n",
    "-------\n",
    "#### View \n",
    "An implementation of a BigQuery View. \n",
    "\n",
    "##### Ancestors (in MRO)\n",
    "- gcp.bigquery._view.View\n",
    "\n",
    "- __builtin__.object\n",
    "\n",
    "##### Instance variables\n",
    "- **description**\n",
    "\n",
    "    The description of the view if it exists.\n",
    "\n",
    "- **friendly_name**\n",
    "\n",
    "    The friendly name of the view if it exists.\n",
    "\n",
    "- **full_name**\n",
    "\n",
    "    The full name of the table.\n",
    "\n",
    "- **name**\n",
    "\n",
    "    The name for the view as a named tuple.\n",
    "\n",
    "- **query**\n",
    "\n",
    "    The View Query.\n",
    "\n",
    "- **schema**\n",
    "\n",
    "    Retrieves the schema of the table.\n",
    "  \n",
    "    Returns:    \n",
    "A Schema object containing a list of schema fields and associated metadata.  \n",
    "Raises  \n",
    "Exception if the request could not be executed or the response was malformed.\n",
    "\n",
    "##### Methods\n",
    "- **__init__** (self, api, name)\n",
    "\n",
    "    Initializes an instance of a View object.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * api: the BigQuery API object to use to issue requests.\n",
    "\n",
    "    * name: the name of the view either as a string or a 3-part tuple (projectid, datasetid, name).\n",
    "\n",
    "- **create** (self, query)\n",
    "\n",
    "    Create the view with the specified query.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * query: the query to use to for the View; either a string or a Query.  \n",
    "Returns:    \n",
    "The View instance.  \n",
    "Raises:    \n",
    "Exception if the view couldn't be created or already exists and overwrite was False.\n",
    "\n",
    "- **delete** (self)\n",
    "\n",
    "    Remove the View if it exists.\n",
    "\n",
    "- **execute** (self, table_name=None, append=False, overwrite=False, use_cache=True, batch=True, timeout=0)\n",
    "\n",
    "    Materialize the View asynchronously.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * dataset_id: the datasetId for the result table.\n",
    "\n",
    "    * table: the result table name; if None, then a temporary table will be used.\n",
    "\n",
    "    * append: if True, append to the table if it is non-empty; else the request will fail if table\n",
    "is non-empty unless overwrite is True.\n",
    "\n",
    "    * overwrite: if the table already exists, truncate it instead of appending or raising an  \n",
    "Exception.\n",
    "\n",
    "    * use_cache: whether to use past results or ignore cache. Has no effect if destination is\n",
    "specified.\n",
    "\n",
    "    * batch: whether to run this as a batch job (lower priority) or as an interactive job (high\n",
    "priority, more expensive).\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the materialization to complete.  \n",
    "Returns:    \n",
    "A Job for the materialization  \n",
    "Raises:    \n",
    "Exception (KeyError) if View could not be materialized.\n",
    "\n",
    "- **exists** (self)\n",
    "\n",
    "    Whether the view has been created.\n",
    "\n",
    "- **results** (self, timeout=0, use_cache=True)\n",
    "\n",
    "    Materialize the view synchronously.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not. Ignored if append is specified.  \n",
    "Returns:    \n",
    "A QueryResultsTable containing the result set.  \n",
    "Raises:    \n",
    "Exception if the query could not be executed or query response was\n",
    "malformed.\n",
    "\n",
    "- **sample** (self, fields=None, count=5, sampling=None, timeout=0, use_cache=True)\n",
    "\n",
    "    Retrieves a sampling of data from the table.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * fields: an optional list of field names to retrieve.\n",
    "\n",
    "    * count: an optional count of rows to retrieve which is used if a specific\n",
    "sampling is not specified.\n",
    "\n",
    "    * sampling: an optional sampling strategy to apply to the table.\n",
    "\n",
    "    * timeout: duration (in milliseconds) to wait for the query to complete.\n",
    "\n",
    "    * use_cache: whether to use cached results or not.  \n",
    "Returns:    \n",
    "A QueryResults object containing the resulting data.  \n",
    "Raises:    \n",
    "Exception if the sample query could not be executed or query response was malformed.\n",
    "\n",
    "- **update** (self, friendly_name=None, description=None, query=None)\n",
    "\n",
    "    Selectively updates View information.\n",
    "  \n",
    "    Args:  \n",
    "\n",
    "    * friendly_name: if not None, the new friendly name.\n",
    "\n",
    "    * description: if not None, the new description.\n",
    "\n",
    "    * expiry: if not None, the new expiry time, either as a DateTime or milliseconds since epoch.\n",
    "\n",
    "    * query: if not None, a new query string for the View.\n",
    "  \n",
    "    Returns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
